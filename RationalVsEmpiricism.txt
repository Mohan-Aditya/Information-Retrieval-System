 NARAYANA SHANMUKHA VENKAT
 Roll no: 17075036
 B Tech Computer Science Engineering. 


To answer this question, we need to understand the reason behind the triumph of Empiricism over Rationalism in recent years.

The rationalist approach had two advantages(the reason behind its initial projection).
First, it is more definite than the "laws of thought" approach. Second, it was more submissive to the scientific community than procedures based on thought, because they knew that the standard of rationality is well defined. 
Empiricism; On the other hand, is well adapted for specific environments since it is the result of an evolutionary process which might still be lacking perfection.
Therefore, regardless of the apparent simplicity with which the problem can be addressed, a tremendous variety of issues develop when we try to solve the problem. We should understand that we will get difficulties long before achieving this rationality. The computational requirements are too expensive. Nevertheless, a hypothesis with clear decision making is a good place to start. It at least will lay a foundation for the problem.
The recent emergence of very diverse, humongous and well-labeled datasets like ImageNet is also a reason behind the rise of Empiricist approach as the risk of overfitting to the dataset is low.
Advances in parallel computing and the increase in memory availability were not efficient enough to meet the requirements of the rationalist approach of a lot of problems which were elegantly solved by Empiricist approach.

Another reason why models with good theoretical background may not translate into real-world is that the assumptions are often known to be false.
Consider, as an example, Latent Dirichlet Allocation (LDA) that I'm presently working on in my Exploratory Project.
It is a notable algorithm in topic modeling and Information retrieval. 
But the proof of LDA is asserted on the assumption that a corpus is correlated with a spread over topics.
Each topic is itself attached to some vocabulary of the corpus. 
Certainly, this assumption cannot endure in any real-world dataset. In reality, words are chosen context based and depend on the sentences in which they are. Also, document lengths aren't always predetermined. 

Although the rationalist approach would have made some progress without the empiricist approach in machine learning, It wouldn't sustain much longer if approaches continue to be rationalist alone.
